{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n"
      ],
      "metadata": {
        "id": "IQd_nqv-0w-G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNYvNt_rxN1g",
        "outputId": "639f2104-8512-4d34-b090-1cdc0f4373e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 words in disaster tweets:\n",
            "[('the', 261406), (',', 209886), ('.', 187369), ('a', 159126), ('and', 117985), ('of', 110122), ('to', 109426), ('/', 83091), ('>', 82985), ('<', 82938), ('br', 82900), ('is', 82287), ('it', 81345), ('i', 74405), ('in', 69840), ('this', 65008), ('that', 59420), (\"'s\", 47238), ('movie', 45174), ('wa', 44364)]\n",
            "\n",
            "Top 20 words in normal tweets:\n",
            "[('the', 270822), (',', 224874), ('.', 185001), ('a', 171211), ('and', 140884), ('of', 121152), ('to', 104607), ('is', 91226), ('it', 82089), ('in', 78582), ('/', 77967), ('>', 77849), ('<', 77783), ('br', 77754), ('i', 64626), ('that', 55412), ('this', 55297), (\"'s\", 50786), ('film', 38435), ('with', 36492)]\n",
            "\n",
            "Top 20 bigrams in disaster tweets:\n",
            "[(('<', 'br'), 82900), (('br', '/'), 82900), (('/', '>'), 82900), (('>', '<'), 41457), (('of', 'the'), 28207), (('.', 'the'), 22213), ((',', 'and'), 21243), (('in', 'the'), 19582), (('.', 'i'), 18596), ((',', 'but'), 17241), (('this', 'movie'), 14115), (('it', \"'s\"), 13292), ((',', 'the'), 13151), (('.', 'it'), 11223), (('the', 'movie'), 10838), (('to', 'be'), 10663), (('the', 'film'), 9838), ((',', 'i'), 9735), (('and', 'the'), 9646), (('to', 'the'), 8657)]\n",
            "\n",
            "Top 20 trigrams in disaster tweets:\n",
            "[(('<', 'br', '/'), 82900), (('br', '/', '>'), 82900), (('/', '>', '<'), 41457), (('>', '<', 'br'), 41457), (('/', '>', 'the'), 5939), (('.', '<', 'br'), 4663), (('/', '>', 'i'), 3821), (('.', 'it', \"'s\"), 3461), (('one', 'of', 'the'), 3163), (('i', 'do', \"n't\"), 2618), ((',', 'and', 'the'), 2600), (('!', '!', '!'), 2476), (('this', 'movie', 'is'), 2423), (('.', 'this', 'is'), 2228), ((',', 'it', \"'s\"), 2164), (('*', '*', '*'), 2139), (('of', 'the', 'movie'), 2035), (('of', 'the', 'film'), 1896), ((',', 'but', 'i'), 1782), ((',', 'but', 'it'), 1779)]\n",
            "\n",
            "Top 20 bigrams in normal tweets:\n",
            "[(('<', 'br'), 77752), (('br', '/'), 77752), (('/', '>'), 77752), (('>', '<'), 38903), (('of', 'the'), 32851), ((',', 'and'), 25504), (('.', 'the'), 20884), (('in', 'the'), 20110), (('.', 'i'), 17444), ((',', 'but'), 16114), ((',', 'the'), 13635), (('it', \"'s\"), 13310), (('is', 'a'), 12789), (('.', 'it'), 12491), (('the', 'film'), 11644), (('and', 'the'), 11298), (('this', 'movie'), 10668), (('to', 'the'), 10031), (('it', 'is'), 9378), ((',', 'a'), 8709)]\n",
            "\n",
            "Top 20 trigrams in normal tweets:\n",
            "[(('<', 'br', '/'), 77752), (('br', '/', '>'), 77752), (('>', '<', 'br'), 38900), (('/', '>', '<'), 38898), (('/', '>', 'the'), 5830), (('.', '<', 'br'), 4845), (('one', 'of', 'the'), 4627), (('.', 'it', \"'s\"), 3792), (('/', '>', 'i'), 3161), (('.', 'this', 'is'), 2967), ((',', 'and', 'the'), 2881), (('!', '!', '!'), 2604), (('.', 'it', 'is'), 2503), (('this', 'is', 'a'), 2412), (('it', \"'s\", 'a'), 2226), (('of', 'the', 'film'), 2174), ((',', 'it', \"'s\"), 2047), (('a', 'lot', 'of'), 2013), (('/', '>', 'this'), 1956), ((',', 'but', 'it'), 1931)]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    df = pd.read_csv('/content/review.csv')\n",
        "except pd.errors.ParserError as e:\n",
        "    print(\"ParserError occurred:\", e)\n",
        "\n",
        "\n",
        "disaster_tweets = df[df['sentiment'] == 'negative']['review'].tolist()\n",
        "normal_tweets = df[df['sentiment'] == 'positive']['review'].tolist()\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "disaster_words = [word_tokenize(tweet.lower()) for tweet in disaster_tweets]\n",
        "disaster_words_lemmatized = [[lemmatizer.lemmatize(word) for word in words] for words in disaster_words]\n",
        "\n",
        "\n",
        "normal_words = [word_tokenize(tweet.lower()) for tweet in normal_tweets]\n",
        "normal_words_lemmatized = [[lemmatizer.lemmatize(word) for word in words] for words in normal_words]\n",
        "\n",
        "disaster_words_flat = [word for sublist in disaster_words_lemmatized for word in sublist]\n",
        "normal_words_flat = [word for sublist in normal_words_lemmatized for word in sublist]\n",
        "\n",
        "disaster_word_freq = FreqDist(disaster_words_flat)\n",
        "normal_word_freq = FreqDist(normal_words_flat)\n",
        "\n",
        "top_disaster_words = disaster_word_freq.most_common(20)\n",
        "top_normal_words = normal_word_freq.most_common(20)\n",
        "\n",
        "disaster_bigrams = list(ngrams(disaster_words_flat, 2))\n",
        "disaster_trigrams = list(ngrams(disaster_words_flat, 3))\n",
        "\n",
        "normal_bigrams = list(ngrams(normal_words_flat, 2))\n",
        "normal_trigrams = list(ngrams(normal_words_flat, 3))\n",
        "\n",
        "top_disaster_bigrams = FreqDist(disaster_bigrams).most_common(20)\n",
        "top_disaster_trigrams = FreqDist(disaster_trigrams).most_common(20)\n",
        "\n",
        "top_normal_bigrams = FreqDist(normal_bigrams).most_common(20)\n",
        "top_normal_trigrams = FreqDist(normal_trigrams).most_common(20)\n",
        "\n",
        "\n",
        "print(\"Top 20 words in disaster tweets:\")\n",
        "print(top_disaster_words)\n",
        "\n",
        "print(\"\\nTop 20 words in normal tweets:\")\n",
        "print(top_normal_words)\n",
        "\n",
        "print(\"\\nTop 20 bigrams in disaster tweets:\")\n",
        "print(top_disaster_bigrams)\n",
        "\n",
        "print(\"\\nTop 20 trigrams in disaster tweets:\")\n",
        "print(top_disaster_trigrams)\n",
        "\n",
        "print(\"\\nTop 20 bigrams in normal tweets:\")\n",
        "print(top_normal_bigrams)\n",
        "\n",
        "print(\"\\nTop 20 trigrams in normal tweets:\")\n",
        "print(top_normal_trigrams)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#remove @ punctuation stop words etc.\n",
        "def preprocess_tweet(tweet):\n",
        "    tweet = re.sub(r'@[A-Za-z0-9_]+', '', tweet)\n",
        "    tweet = re.sub(r'[^a-zA-Z0-9\\s]', '', tweet)\n",
        "\n",
        "\n",
        "    words = word_tokenize(tweet.lower())\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "disaster_tweets_preprocessed = [preprocess_tweet(tweet) for tweet in disaster_tweets]\n",
        "normal_tweets_preprocessed = [preprocess_tweet(tweet) for tweet in normal_tweets]\n",
        "\n",
        "print(\"Sample disaster tweets after preprocessing:\")\n",
        "for tweet in disaster_tweets_preprocessed[:5]:\n",
        "    print(\"-\", tweet)\n",
        "\n",
        "print(\"\\nSample normal tweets after preprocessing:\")\n",
        "for tweet in normal_tweets_preprocessed[:5]:\n",
        "    print(\"-\", tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHoy2vr_zR68",
        "outputId": "433f7c83-1b22-4fcc-9831-8fec3b461f4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample disaster tweets after preprocessing:\n",
            "- argued whether rent im always afraid renting something ive never heard dont remember theater great castthats tipped scale 30 minute almost stopped watching first minute fun watch unbelievable get worse writer movie could little research future project want make movie even little better could try writing something little bit believable give 3a 1 writing wordsand 2 able get many good actor agree movie despite read script oh god movie suck\n",
            "- one dullest movie seen time im late 40 watched soninlaw early 20 son 17 scenery beautiful story bust watched hour turned spent time iphone hour watched spent actually watching movie gave 3 enjoyed scenery cinematography otherwise would given 1 im sure people really art find review appalling entitled opinion right couldnt figure supposed chick flick focus mother supposed movie guy focus battle adventure opinion didnt succeed either\n",
            "- funny thing happening sitcom based main character jim either bad father bad husband generally enormously selfish funny course character sitcom flawed jims character flawed extremely unsympathetic mannerbr br guess better jims stupid guy take care thing he got opportunity chooses conscious choice make chooses play kid go shopping doesnt want buy lady product choice make put relativesbr br character seems series jim someone jerk cheryl character real person would left year ago stay deadbeat 8 year ala shes catalyst jims quirky middleclass extreme selfishness\n",
            "- know movie never complete justice book exceptional important character cut blanca alba essentially mushed character subplots major element main plot eliminated claras clairvoyance extremely downplayed making seem like much shallow character one got know book book learn power important effect many people turn key element life family movie special lady relationship esteban pedro tercero tercerothird way son thus come segundosecond connection esteban grandson pancha garca son also recognize chopped half importance downplayedbr br one fundamental thing book film stripped called house spirit house story 34 generation family supposed revolve around big house corner line stated many time novel house fundamental story movie unjustly relegates mere backdropbr br hadnt read book would never guessed sappy shallow movie could based rich entertaining novel\n",
            "- film universal release protelcomlc production boring retelling theory breaking molecular structure object capturing cell pure energy sending back complete target area explanation necessary professor paul steiner played pockmocked actor bryant haliday devil doll think something dedicate assistant pat hill mary peach chris mitchell ronald allen life br br experiment noted dutch scientist lembach gordon heinz machine fails due sabotage projected secretary sheila tracey crisp seek revenge course screw come looking like pork roast power electrocute people br br newfound power manages zap cockney idiot security guy named latham derrick de marney lab bos dr blanchard norman woodland also able break pharmacy steal pair rubber glove black coat well br br end though despite hill mitchell attempt help clown destroys equipment whole completely pointless movie message br br also one depressing color film ever see\n",
            "\n",
            "Sample normal tweets after preprocessing:\n",
            "- didnt know expect ned kelly absolutely loved dark dramatic gripping also felt authentic felt transported back 1800s ive never much heath ledger fan seen teen type movie however quite compelling role ledger play ned kelly dignity intensity showing u highly spirited boy became australia notorious killer naomi watt great supporting role kelly society lover highly recommended thats aussie\n",
            "- read book saw movie knew movie going good book great seriously recommend see amazing fantastic movie know like went see sister nobody u little disappointed nobody know gone movie came saying horrible movie nobody great think everyone like bad nobody want see anyway hope heard movie make want see movie guarantee youll like much im obsessed literlly\n",
            "- ultimate chick flick ever taped tv year ago ive watched 30 time year hadnt seen 12 year recently watched movie im lying cried opening credit ending credit movie truly tear heart even dont child\n",
            "- lighthearted comedy nothing show u world sometimes wish escape world nothing anything dont like stack bill bad memory even hunger disappear wish approached movie well enjoyable starring duo thing didnt like nothing werent even part main moviebr br first postcredits scene yes one good chuckle trying accomplish confused eager see return something whole movie nothing instead hear random assortment noise scream try set sequel opinion wasnt really necessary funny turtle crawled framebr br second trailer saw trailer dvd like others already said promotes horror movie never came oh well poor marketing guessbr br see movie rental store take look nothing great movie watch big screen though might want wear shade\n",
            "- hadnt heard lot movie except national award oscar entry marathi movie n cant make apple orange marathi saw movie playing dd1 late sunday night got glued judge cinematic technique acting skill watched good number movie various genre average viewer highly recommend movie feel earthy realistic though melodramatic moment watch feel human lately havent seen movie touch heart especially hindi cinema crowning achievement movie young kid return home camera move around reveal kid wearing black glass lost eye kid hears kid splashing water start clapping awestruck two hour spent watching movie much worth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_preprocessed = pd.read_csv('/content/review.csv')\n",
        "\n",
        "\n",
        "X = df_preprocessed['review']\n",
        "y = df_preprocessed['sentiment']\n",
        "\n",
        "\n",
        "vectorizer_100 = CountVectorizer(max_features=100)\n",
        "X_vectorized_100 = vectorizer_100.fit_transform(X)\n",
        "vectorizer_1000 = CountVectorizer(max_features=1000)\n",
        "X_vectorized_1000 = vectorizer_1000.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train_100, X_test_100, y_train, y_test = train_test_split(X_vectorized_100, y, test_size=0.2, random_state=42)\n",
        "X_train_1000, X_test_1000, y_train, y_test = train_test_split(X_vectorized_1000, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "logreg_100 = LogisticRegression(max_iter=1000)\n",
        "logreg_100.fit(X_train_100, y_train)\n",
        "\n",
        "\n",
        "y_pred_100 = logreg_100.predict(X_test_100)\n",
        "accuracy_100 = accuracy_score(y_test, y_pred_100)\n",
        "f1_100 = f1_score(y_test, y_pred_100, average='weighted')\n",
        "recall_100 = recall_score(y_test, y_pred_100, average='weighted')\n",
        "\n",
        "\n",
        "logreg_1000 = LogisticRegression(max_iter=1000)\n",
        "logreg_1000.fit(X_train_1000, y_train)\n",
        "\n",
        "\n",
        "y_pred_1000 = logreg_1000.predict(X_test_1000)\n",
        "accuracy_1000 = accuracy_score(y_test, y_pred_1000)\n",
        "f1_1000 = f1_score(y_test, y_pred_1000, average='weighted')\n",
        "recall_1000 = recall_score(y_test, y_pred_1000, average='weighted')\n",
        "\n",
        "\n",
        "print(\"Performance with max_features=100:\")\n",
        "print(\"Accuracy:\", accuracy_100)\n",
        "print(\"F1 Score:\", f1_100)\n",
        "print(\"Recall:\", recall_100)\n",
        "\n",
        "print(\"\\nPerformance with max_features=1000:\")\n",
        "print(\"Accuracy:\", accuracy_1000)\n",
        "print(\"F1 Score:\", f1_1000)\n",
        "print(\"Recall:\", recall_1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUStn1_az4kH",
        "outputId": "da851d32-7dc7-4d48-a561-d09274f72e0b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance with max_features=100:\n",
            "Accuracy: 0.733625\n",
            "F1 Score: 0.7335920477969484\n",
            "Recall: 0.733625\n",
            "\n",
            "Performance with max_features=1000:\n",
            "Accuracy: 0.864625\n",
            "F1 Score: 0.8646381045897199\n",
            "Recall: 0.864625\n"
          ]
        }
      ]
    }
  ]
}